"""
Inference script for the BESSTIE figurative language project.

This module provides functions to load a fine‑tuned model and generate
predictions for new text data.  Predictions can be run on arbitrary lists
of strings or on a CSV file with a ``text`` column.  The output can be
written to a new CSV file with an additional ``prediction`` column where
``0`` indicates non‑sarcastic / negative and ``1`` indicates sarcastic / positive.
"""

import argparse
import os
from typing import List

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification


def predict_binary(
    checkpoint_dir: str,
    texts: List[str],
    device: str = "auto",
    batch_size: int = 32,
    max_length: int = 256,
    fp16: bool = False,
    bf16: bool = False,
) -> List[int]:
    """Generate binary predictions for a list of texts using a saved model.

    Parameters
    ----------
    checkpoint_dir : str
        Directory containing the fine‑tuned model files (generated by
        ``train.py``).
    texts : List[str]
        List of input sentences to classify.
    device : str, optional
        ``"auto"``, ``"cuda"``, or ``"cpu"``.  ``"auto"`` selects CUDA when available.
    batch_size : int, optional
        Batch size for inference.
    max_length : int, optional
        Maximum sequence length for tokenization.
    fp16 : bool, optional
        Use FP16 mixed precision on CUDA.
    bf16 : bool, optional
        Use BF16 mixed precision on CUDA (requires hardware support).

    Returns
    -------
    List[int]
        Predicted labels: ``1`` for positive / sarcastic and ``0`` for
        negative / non‑sarcastic.
    """
    if fp16 and bf16:
        raise ValueError("Choose only one of fp16 or bf16.")
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    device = torch.device(device)
    use_cuda = device.type == "cuda"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint_dir)
    model.to(device)
    model.eval()
    from torch.utils.data import DataLoader

    def collate(batch_texts: List[str]):
        return tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        )

    loader = DataLoader(texts, batch_size=batch_size, shuffle=False, collate_fn=collate)
    preds: List[int] = []
    autocast_dtype = None
    if use_cuda and fp16:
        autocast_dtype = torch.float16
    elif use_cuda and bf16:
        autocast_dtype = torch.bfloat16
    use_autocast = autocast_dtype is not None
    with torch.inference_mode():
        for batch in loader:
            batch = {k: v.to(device, non_blocking=use_cuda) for k, v in batch.items()}
            with torch.cuda.amp.autocast(enabled=use_autocast, dtype=autocast_dtype):
                logits = model(**batch).logits
            preds.extend(logits.argmax(dim=-1).tolist())
    return preds


def main():
    parser = argparse.ArgumentParser(description="Generate predictions using a fine‑tuned BESSTIE model.")
    parser.add_argument("--checkpoint_dir", type=str, default="./model_output/", help="Directory containing the fine‑tuned model")
    parser.add_argument("--input_file", type=str,default = "../dataset/valid.csv" ,help="CSV file with a 'text' column to predict labels for")
    parser.add_argument("--output_file", type=str, help="Where to save the CSV with predictions (optional)")
    parser.add_argument("--text", type=str, nargs="*", help="One or more texts to classify (if no input_file is provided)")
    parser.add_argument("--device", type=str, default="auto", help="Device: auto, cuda, or cpu")
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for inference")
    parser.add_argument("--max_length", type=int, default=256, help="Max sequence length for tokenization")
    parser.add_argument("--fp16", action="store_true", help="Use FP16 mixed precision on CUDA")
    parser.add_argument("--bf16", action="store_true", help="Use BF16 mixed precision on CUDA")
    args = parser.parse_args()
    if args.input_file:
        if not os.path.exists(args.input_file):
            raise FileNotFoundError(f"Input file {args.input_file} does not exist")
        df = pd.read_csv(args.input_file)
        if "text" not in df.columns:
            raise ValueError("Input CSV must contain a 'text' column")
        texts = df["text"].astype(str).tolist()
        preds = predict_binary(
            args.checkpoint_dir,
            texts,
            device=args.device,
            batch_size=args.batch_size,
            max_length=args.max_length,
            fp16=args.fp16,
            bf16=args.bf16,
        )
        df["prediction"] = preds
        output_path = args.output_file or os.path.splitext(args.input_file)[0] + "_predictions.csv"
        df.to_csv(output_path, index=False)
        print(f"Predictions written to {output_path}")
    else:
        if not args.text:
            raise ValueError("Either --input_file or --text must be provided")
        preds = predict_binary(
            args.checkpoint_dir,
            args.text,
            device=args.device,
            batch_size=args.batch_size,
            max_length=args.max_length,
            fp16=args.fp16,
            bf16=args.bf16,
        )
        for t, p in zip(args.text, preds):
            label = "sarcastic/positive" if p == 1 else "non‑sarcastic/negative"
            print(f"{t} -> {label}")


if __name__ == "__main__":
    main()
